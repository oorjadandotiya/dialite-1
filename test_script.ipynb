{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7830a579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['review', 'sentiment', 'title'], dtype='object')\n",
      "Index(['title', 'review', 'ï»¿review', 'sentiment'], dtype='object')\n",
      "Common columns: ['review', 'sentiment', 'title']\n",
      "True Positives: 150\n",
      "False Positives: 3\n",
      "False Negatives: 250\n",
      "\n",
      "Precision: 98.04%\n",
      "Recall:    37.50%\n",
      "F1 Score:  54.25%\n",
      "\n",
      "Rows in ALITE but not in Ground Truth (False Positives):\n",
      "\n",
      "Rows in Ground Truth but missing from ALITE (False Negatives):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the datasets\n",
    "ground_truth = pd.read_csv(r\"data\\dat_dialite\\Categorical - Review\\groundreview.csv\")\n",
    "alite_output = pd.read_csv(r\"data\\dat_dialite\\Categorical - Review\\ir_alite_Categorical - Reviews.csv\")\n",
    "\n",
    "# Step 1: Standardise column names\n",
    "ground_truth.columns = ground_truth.columns.str.strip().str.lower()\n",
    "alite_output.columns = alite_output.columns.str.strip().str.lower()\n",
    "print(ground_truth.columns)\n",
    "print(alite_output.columns)\n",
    "\n",
    "# Step 2: Identify common columns\n",
    "common_cols = sorted(set(ground_truth.columns) & set(alite_output.columns))\n",
    "print(\"Common columns:\", common_cols)\n",
    "\n",
    "# Step 3: Clean and flatten each row into a tuple of values\n",
    "def clean_and_flatten(df):\n",
    "    return [\n",
    "        tuple(\n",
    "            str(x).strip().lower() if pd.notna(x) else \"missing\"\n",
    "            for x in row\n",
    "        )\n",
    "        for row in df[common_cols].values.tolist()\n",
    "    ]\n",
    "\n",
    "gt_rows = set(clean_and_flatten(ground_truth))\n",
    "ao_rows = set(clean_and_flatten(alite_output))\n",
    "\n",
    "# Step 4: Compare the sets of rows\n",
    "tp = gt_rows & ao_rows  # True Positives\n",
    "fp = ao_rows - gt_rows  # False Positives\n",
    "fn = gt_rows - ao_rows  # False Negatives\n",
    "\n",
    "print(\"True Positives:\", len(tp))\n",
    "print(\"False Positives:\", len(fp))\n",
    "print(\"False Negatives:\", len(fn))\n",
    "\n",
    "# Step 5: Compute metrics\n",
    "precision = len(tp) / (len(tp) + len(fp)) if (len(tp) + len(fp)) > 0 else 0\n",
    "recall = len(tp) / (len(tp) + len(fn)) if (len(tp) + len(fn)) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\nPrecision: {:.2%}\".format(precision))\n",
    "print(\"Recall:    {:.2%}\".format(recall))\n",
    "print(\"F1 Score:  {:.2%}\".format(f1))\n",
    "\n",
    "# Step 6: Display mismatches safely\n",
    "fp_rows = [list(row) for row in fp if len(row) == len(common_cols)]\n",
    "fn_rows = [list(row) for row in fn if len(row) == len(common_cols)]\n",
    "\n",
    "fp_df = pd.DataFrame(fp_rows, columns=common_cols)\n",
    "fn_df = pd.DataFrame(fn_rows, columns=common_cols)\n",
    "\n",
    "print(\"\\nRows in ALITE but not in Ground Truth (False Positives):\")\n",
    "\n",
    "\n",
    "print(\"\\nRows in Ground Truth but missing from ALITE (False Negatives):\")\n",
    "\n",
    "\n",
    "# Step 7: Save metrics\n",
    "metrics = pd.DataFrame({\n",
    "    'Metric': ['Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [precision, recall, f1]\n",
    "})\n",
    "metrics.to_csv(\"analysis/set_row_match_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22e33c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Null Value Analysis ---\n",
      "Total Nulls Before Integration: 24710\n",
      "Total Nulls After Integration: 140\n",
      "Ground Truth Nulls: 140\n",
      "\n",
      "Null Value Score (Integrated Table): -24570\n",
      "Null Value Score (Ground Truth Table): -24570\n",
      "\n",
      "Percentage Change in Nulls (Integrated Table): -99.43%\n",
      "Percentage Change in Nulls (Ground Truth Table): -99.43%\n",
      "\n",
      "Results have been saved to 'null_value_scores.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read your tables\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tableA = pd.read_csv(r\"data\\dat_dialite\\NumericalX2 - Weather\\pressure.csv\")\n",
    "tableB = pd.read_csv(r\"data\\dat_dialite\\NumericalX2 - Weather\\temperature.csv\")\n",
    "integrated_table = pd.read_csv(r\"data\\dat_dialite\\NumericalX2 - Weather\\temp_pressure.csv\")\n",
    "ground_truth_table = pd.read_csv(r\"data\\dat_dialite\\NumericalX2 - Weather\\ground_temp_pressure.csv\")\n",
    "\n",
    "# Define tables before integration\n",
    "tables_before_integration = [tableA, tableB]\n",
    "\n",
    "# Step 1: Calculate total nulls before integration\n",
    "nulls_before_integration = sum(table.isnull().sum().sum() for table in tables_before_integration)\n",
    "\n",
    "# Step 2: Calculate total nulls after integration\n",
    "nulls_after_integration = integrated_table.isnull().sum().sum()\n",
    "\n",
    "# Step 3: Calculate total nulls in ground truth\n",
    "ground_truth_nulls = ground_truth_table.isnull().sum().sum()\n",
    "\n",
    "# Step 4: Calculate Null Value Scores\n",
    "nvs_integration = nulls_after_integration - nulls_before_integration\n",
    "nvs_groundtruth = ground_truth_nulls - nulls_before_integration\n",
    "\n",
    "# Step 5: Output the results\n",
    "print(\"--- Null Value Analysis ---\")\n",
    "print(f\"Total Nulls Before Integration: {nulls_before_integration}\")\n",
    "print(f\"Total Nulls After Integration: {nulls_after_integration}\")\n",
    "print(f\"Ground Truth Nulls: {ground_truth_nulls}\")\n",
    "print()\n",
    "print(f\"Null Value Score (Integrated Table): {nvs_integration}\")\n",
    "print(f\"Null Value Score (Ground Truth Table): {nvs_groundtruth}\")\n",
    "\n",
    "# Optional: Calculate percentage change in nulls\n",
    "if nulls_before_integration > 0:\n",
    "    pct_change_integration = ((nulls_after_integration - nulls_before_integration) / nulls_before_integration) * 100\n",
    "    pct_change_groundtruth = ((ground_truth_nulls - nulls_before_integration) / nulls_before_integration) * 100\n",
    "\n",
    "    print()\n",
    "    print(f\"Percentage Change in Nulls (Integrated Table): {pct_change_integration:.2f}%\")\n",
    "    print(f\"Percentage Change in Nulls (Ground Truth Table): {pct_change_groundtruth:.2f}%\")\n",
    "else:\n",
    "    print(\"No nulls present before integration; percentage change cannot be calculated.\")\n",
    "\n",
    "# Step 6: Save Results to CSV\n",
    "results = {\n",
    "    \"Metric\": [\n",
    "        \"Total Nulls Before Integration\",\n",
    "        \"Total Nulls After Integration\",\n",
    "        \"Ground Truth Nulls\",\n",
    "        \"Null Value Score (Integrated Table)\",\n",
    "        \"Null Value Score (Ground Truth Table)\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        nulls_before_integration,\n",
    "        nulls_after_integration,\n",
    "        ground_truth_nulls,\n",
    "        nvs_integration,\n",
    "        nvs_groundtruth\n",
    "    ]\n",
    "}\n",
    "\n",
    "if nulls_before_integration > 0:\n",
    "    results[\"Metric\"].extend([\n",
    "        \"Percentage Change in Nulls (Integrated Table)\",\n",
    "        \"Percentage Change in Nulls (Ground Truth Table)\"\n",
    "    ])\n",
    "    results[\"Value\"].extend([\n",
    "        pct_change_integration,\n",
    "        pct_change_groundtruth\n",
    "    ])\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"null_value_scores.csv\", index=False)\n",
    "print(\"\\nResults have been saved to 'null_value_scores.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
